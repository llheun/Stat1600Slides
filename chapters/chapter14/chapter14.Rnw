%!Rnw root = ../../Master.Rnw

\section{STAT 1600 Ch 14 Linear Regression}

\begin{frame}[fragile]{Statistics and Data Analysis}

STAT 1600 \\ Ch 14 Linear Regression

\end{frame}

\begin{frame}[fragile]{Outline}

Simple Linear Regression

\begin{itemize}
\item Regression and Straight Line Model  
\item Least Square Solution
\item Fitted-Line Plot
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Why Regression}

For a pair of numerical variables, we want not only to measure the  strength of linear association (i.e., correlation), but also to estimate the  linear relationship and establish a sound straight line model to relate  these two variables.

\end{frame}

\begin{frame}[fragile]{Straight Line Model}

\begin{itemize}
\item<1-> We want to relate a $y$ value (value of a numerical variable) for an $x$ value  (value of another numerical variable) and establish the straight line model:

\item<2->
\begin{equation*}
y = a + bx
\end{equation*}


\item<3-> $y$ is called the response variable
\item<4-> $x$ is called a explanatory or predictor variable
\item<5-> $a$ and $b$ are, respectively, the ($y$-)intercept and the slope of the straight line.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Simple Linear Regression Model}

\begin{itemize}
\item<1-> For a sample of $n$ pairs of the numerical variable values, the $x$ values  and the $y$ values usually do not follow exactly the straight line pattern.  For instance, in relating son's height ($y$) with father's height ($x$), fathers  of the same height may have sons of different heights. Consequently,  the straight line may better be described as

\vspace{-3mm}

\begin{equation*}
y = a + bx + e
\end{equation*}

\item<2-> $e$ is the error which represents the deviation of an individual $y$
value from the straight line
\item<3-> smaller errors overall imply a tighter linear pattern
\end{itemize}

\end{frame}

\begin{frame}[fragile]{iClicker Question 14.1}

{\small{Data were collected for the average public school teacher pay and  spending on public schools per pupil in year 1985 for the Northeast  and North Central states. Suppose we want to predict the average  public school teacher pay from the spending on public schools per  pupil. Which of the following statements is true?


\begin{enumerate}[A]
\item Spending is the response and teacher pay is the  explanatory variable.
\item Both spending and teach pay are response variables.
\item Teacher pay is the response and spending is the  explanatory variable.
\item Both spending and teacher pay are explanatory variables.
\item None of the previous is true.
\end{enumerate}
}}
\end{frame}

\begin{frame}[fragile]{iClicker Question 14.2}

{\footnotesize{
A child's first year of life is often said to be more important to development than any other. A critical child psychologist wondered if this was true, so she decided to collect some real data that might help her find out. She asked several hundred parents of 18 year old's how many hours they spent reading to their children per week over the first year of life, and recorded the high school GPA's of all the children. If she fits a least square regression line to her data, what should she select as her explanatory and response variables?

\begin{enumerate}[A]
\item High school GPA is explanatory, and hours reading is response.
\item Quality of parents is explanatory, and high school GPA is response.
\item High school GPA is explanatory, and quality of parents Is response.
\item Hours reading is explanatory, and high school GPA is response.
\end{enumerate}
}}
\end{frame}

\begin{frame}[fragile]{Predicted Value and Residual}

{\small{
\begin{itemize}
\item<1-> \textbf{Predicted value}. For a given $X$ value, we use the straight line  model to `predict' the associated $Y$ value. Denote
PRED = PREDICTED $Y$ the \textit{predicted} (or estimated or fitted)  \textit{mean} $Y$ value. We call it \textbf{predicted} value (or fitted value). That  is,

\vspace{-1mm}

{\footnotesize{
\begin{equation*}
PRED = a + bX
\end{equation*}
}}

\vspace{-6mm}

\item<2-> \textbf{Residual}. A residual is the difference (or deviation) between a $Y$ value and a predicted value. That is, the residual is computed by

\vspace{-3mm}

{\footnotesize{
\begin{equation*}
\texttt{RESIDUAL} = Y - \texttt{PREDICTED} Y
\end{equation*}
}}

\vspace{-4mm}

\item<3-> A good fit of the data to the model is one with reasonably small  residuals.
\end{itemize}
}}
\end{frame}

\begin{frame}[fragile]{Method of Least Squares}

{\footnotesize{
\begin{itemize}
\item<1-> Least squares solution to a straight line model.

{\footnotesize{
\begin{eqnarray*}
\hat{b} &=& r \times \frac{SD_y}{SD_x} \\
\hat{a} &=& \bar{Y} - \hat{b} \bar{X}
\end{eqnarray*}
}}

\vspace{-6mm}

and hence,

\vspace{-3mm}

{\footnotesize{
\begin{eqnarray*}
\texttt{PREDICTED Y} &=& \hat{a} + \hat{b} X \\
\texttt{RESIDUAL} &=& Y - \texttt{PREDICTED} Y
\end{eqnarray*}
}}

\vspace{-6mm}

\item<2-> Residual sum of squares. The residual sum of squares,  denoted SSE (i.e., Sum of Squared Errors), is the sum of the  squared residuals which reflects the total variation not captured by  the model.
\item<3-> Method of least squares. The method of least squares produces  minimal residual sum of squares.
\end{itemize}
}}
\end{frame}

\begin{frame}[fragile]{Saturn Price Example}

Proceeding with the computation procedure for the data, we have the  mean and the standard deviation for Miles: $X = 61,195$ and $SD_x = 50,989$; and the mean and the standard deviation for Price:  $Y = \$4,999$ and $SD_y = \$4,079$; and the correlation between Miles and  Price: $r = -0.641$.  Hence, the least squares solution of the straight  line:

{\footnotesize{
\begin{eqnarray*}
\texttt{slope: } \hat{b} &=& -0.641 \times \frac{4079}{50989} = -0.05127 \\
\texttt{intercept: } \hat{a} &=& 4999 - (-0.05127) \times 61195 = 8136
\end{eqnarray*}
}}
\end{frame}

\begin{frame}[fragile]{Interpretation of Slope and Intercept}

\begin{itemize}
\item<1-> Cautions must be exercised in interpreting the slope and the intercept:
\item<2-> The increase by 1 unit in $x$ value is, on average, associated with $\hat{b}$ units increase/decrease in $y$. The use of wording such as `causes`  is INCORRECT.
\item<3-> If zero is not included in the range of the $x$ data values, then there  is no \textit{practical} explanation of the intercept ($\hat{a}$).
\item<4-> The straight line equation is used to `predict' y value for $x$ value  which is within the $x$ data range. It is not to be used to `forecast' $y$  value for which $x$ value falls beyond the $x$ data range.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Saturn Price Example, cont'd}

Recall that the estimated slope and intercept are

\begin{itemize}
\item<1-> 
$\texttt{Slope } \hat{b} = -0.05127$ per Mile and $\texttt{Intercept } \hat{a} = \$8,136$

\item<2->   and hence the least square line is
{\footnotesize{
\begin{equation*}
\texttt{PREDICTED PRICE} = 8136 + (0.05127) \times \texttt{MILES}
\end{equation*}
}}

\vspace{-6mm}

\item<3-> \textbf{Slope}: the predicted Price tends to drop about 5 cents for every  additional mile driven, or about \$512.70 for every 10,000 miles.

\item<4-> \textbf{Intercept}: should NOT be interpreted as the predicted price of a car with zero (0) mileage.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{iClicker Question 14.3}

{\footnotesize{
Data were collected for the average public school teacher pay (\$) and  spending (\$) on public schools per pupil in year 1985 for the Northeast  and North Central states. The LS equation is

{\footnotesize{
\begin{equation*}
Pay = \$10,670 + \$3.53 \texttt{ Spending.}
\end{equation*}
}}

\vspace{-3mm}

Which of the following statements is \textbf{false}?

\begin{enumerate}[A]
\item The average public school teacher pay increases \$3.53  on average for an additional dollar spending on public  schools per pupil.
\item The correlation between spending and pay is positive. 
\item There is a upward linear relationship between spending and pay. 
\item Interpretation of the intercept: the teachers get an average pay of  \$10,670 even when there is zero dollar spending.
\end{enumerate}
}}
\end{frame}

\begin{frame}[fragile]{iClicker Question 14.4}

{\footnotesize{
Data were collected for the average public school teacher pay (\$) and  spending (\$) on public schools per pupil in year 1985 for the Northeast  and North Central states. The LS equation is

{\footnotesize{
\begin{equation*}
Pay = \$10,670 + \$3.53 \texttt{ Spending.}
\end{equation*}
}}

\vspace{-3mm}

Which of the following statements is \textbf{false}?

\begin{enumerate}[A]
\item The average public school teacher pay increases 3.53 on  average for an additional dollar spending on public  schools per pupil.
\item The correlation between spending and pay is positive.
\item A \$1 increase in spending causes \$3.53 increase in pay.
\item There is a upward linear relationship between spending  and pay.
\end{enumerate}
}}
\end{frame}

\begin{frame}[fragile]{Calculate Predicted Values}

{\footnotesize{
The predicted value of the response, PREDICTED Y or $\hat{Y}$, for a `new' $X$ value (within $X$ data range) can be calculated:

{\footnotesize{
\begin{equation*}
\texttt{PREDICTED Y or } \hat{Y} = \hat{a} + \hat{b} \cdot X
\end{equation*}
}}

\vspace{-5mm}

For the Saturn Price example, the straight line model is:

{\footnotesize{
\begin{equation*}
\texttt{PREDICTED Y } = 8136 + (-0.05127) \cdot X
\end{equation*}
}}

\vspace{-6mm}

\begin{itemize}
\item This is valid for cars with driven miles contained in the $X$ data range.  \item Therefore, the predicted sales price for a car with $Miles = 100,000$ (this value is in range) can be calculated by

\vspace{-4mm}

{\footnotesize{
\begin{equation*}
\texttt{PRED Y } = 8136 + (-0.05127) \times 100,000 = 8136 - 5187 = 2949
\end{equation*}
}}

\vspace{-4mm}

that is, a predicted price of \$2,949.

\end{itemize}
}}
\end{frame}

\begin{frame}[fragile]{Fitted-Line Plot}

{\small{
To get a fitted-line plot (LS line superimposed on the scatterplot):
}}

\begin{itemize}
\item<1-> Produce a scatterplot.
\item<2-> Calculate the predicted response values at the two extreme $X$ values in the range:

{\footnotesize{
\begin{eqnarray*}
  PRED_{min} &=& \hat{a} + \hat{b} X_{min} \\
  PRED_{max} &=& \hat{a} + \hat{b} X_{max}
\end{eqnarray*}
}}

\vspace{-6mm}

\item<3-> Mark these two points ($X_{min}, PRED_{min}$) and ($X_{max}, PRED_{max}$ ) on  the scatterplot and then connect them with a line segment.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Saturn Price Example, revisited}

{\small{
The fitted-line plot for using Least Squares solution is given:
}}

\vspace{-6mm}

<<label=LBL14a, results="asis", echo=FALSE, out.width="7cm">>=
  dtMiles1 <- c(9300,10565,15000,15000,17764,57000,65940,73676,77006,93739,146088,153260)
  dtPrc1   <- c(7100,15500,4400,4401,5900,4600,8800,2000,2750,2550,960,1025)
  plot(dtMiles1, dtPrc1, xlab="Miles", ylab="Price")
@

\end{frame}

\begin{frame}[fragile]{Saturn Price Example, revisited}

{\small{
The fitted-line plot for using Least Squares solution is given:
}}

%\begin{minipage}[ht]{5cm}

<<label=LBL14b, results="asis", echo=FALSE, out.width="4.5cm">>=
  lsm1 <- lm(dtPrc1 ~ dtMiles1)
  
  yhatmx1 <- lsm1$coefficients[1] + lsm1$coefficients[2] * max(dtMiles1)
  yhatmn1 <- lsm1$coefficients[1] + lsm1$coefficients[2] * min(dtMiles1)
  
  plot(dtMiles1, dtPrc1, xlab="Miles", ylab="Price")
  lines( c(min(dtMiles1), max(dtMiles1)), c( yhatmn1, yhatmx1) )
  tmpco1 <- sprintf("%.0f", lsm1$coefficients[1])
  tmpco2 <- sprintf("%.4f", lsm1$coefficients[2])
  tmpY1  <- sprintf("%.0f", yhatmx1)
  tmpY2  <- sprintf("%.0f", yhatmn1)
  tmpmxM1 <- sprintf("%.0f", max(dtMiles1))
@

\vspace{-3mm}

{\footnotesize{
\begin{eqnarray*}
  PRED_{forMaxX} &=& \Sexpr{tmpco1} + (\Sexpr{tmpco2}) * \Sexpr{tmpmxM1} = \Sexpr{tmpY1} \\
  PRED_{forMinX} &=& \Sexpr{tmpco1} + (\Sexpr{tmpco2}) * \Sexpr{min(dtMiles1)} = \Sexpr{tmpY2}
\end{eqnarray*}
}}

\end{frame}

\begin{frame}[fragile]{Outline}

Fitted Values and Residuals

\begin{itemize}
\item Calculate Fitted Values and Residuals
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Calculate Fitted Values and Residuals}

\begin{itemize}

\item Recall that the fitted equation is
{\footnotesize{
\begin{equation*}
Price = \Sexpr{tmpco1} + (\Sexpr{tmpco2}) \times Miles 
\end{equation*}
}}

\vspace{-3mm}

\item For car \#6, the (Miles,Price) pair was (57000, 4600). The fitted value is

{\footnotesize{
\begin{equation*}
\hat{y} = \Sexpr{tmpco1} + (\Sexpr{tmpco2}) \times 57000 = 5213.61
\end{equation*}
}}

\vspace{-3mm}

\item The residual is then:

{\footnotesize{
\begin{equation*}
residual = observed - fitted = 4600 - 5212.61 = -613.61 
\end{equation*}
}}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Outline}

Confidence Interval of the Slope

\begin{itemize}
\item Calculate the Confidence Interval fo the Slope
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Confidence Interval of the Slope}

\begin{itemize}
\item Standard error of the slope:
\begin{eqnarray*}
SE &=& \sqrt{ \frac{ 1 - (\texttt{correlation})^2}{n - 2}} \times \frac{SD_y}{SD_x} \\
   &=& \sqrt{ \frac{ (1 - r^2)}{r^2 (n - 2)}} \times \texttt{ slope}
\end{eqnarray*}
  
where $r$ = correlation, $\hat{b}$ = slope, and $n$ = number of (x,y) pairs.  

\item The margin of error 
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Confidence Interval of the Slope}

\begin{itemize}
  \item The margin of error is $$ME = 1.96 \times SE$$
  \item 95\% confidence interval for the slope:
  \begin{equation*}
    (\hat{b} \pm SE)
  \end{equation*}
  
\item The slope is statistically \textbf{significant} if the interval \textbf{excludes} zero (0). The  slope is \textbf{insignificant} if the interval includes zero (0).
  
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Saturn Price Example, revisited}

{\small{
\begin{itemize}
\item Standard error of the slope:
{\footnotesize{
\begin{eqnarray*}
SE &=& \sqrt{ \frac{ 1 - (-0.641)^2}{(-0.641)^2 (12 - 2)}} \times (-0.05127) \\
   &=& \sqrt{ \frac{ (1 - 0.41088)}{0.41088 \times 10}} \times (-0.05127) = 0.0194
\end{eqnarray*}
}}
\item The margin of error $$ ME = 1.96 \times 0.0194 = 0.0380 $$

\item 95\% CI for the slope:
\begin{equation*}
(-0.05127) \pm 0.0380) \Rightarrow (-0.0893, -0.0133)
\end{equation*}

\item The CI excludes 0 and consequently, the linear relationship between mileage and selling price is significant.

\end{itemize}
}}
\end{frame}

\begin{frame}[fragile]{iClicker Question 14.2.1}

The correlation and the slope of the LS equation from $n = 27$ pairs of  two numerical variables are -0.80 and -1.20. Which of the following  is the standard error of the slope?

\begin{enumerate}[A]
\item 0.18
\item -0.18
\item 0
\item 18  
\item -18
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{iClicker Question 14.2.2}

{\footnotesize{
Data were collected for the average public school teacher pay (\$) and  spending (\$) on public schools per pupil in year 1985 for the Northeast  and North Central states. A 95\% confidence interval for the slope is  (2.148, 4.912). Which of the following statements is true?

\begin{enumerate}[A]
\item The interval excludes 0 and hence the LS equation  provides significant predicting power.
\item The interval includes 0 and hence the LS equation does  not provide significant predicting power.
\item The interval excludes 0 and hence the LS equation does  not provide significant predicting power.
\item The interval includes 0 and hence the LS equation  provides significant predicting power.
\item None of the previous is true.
\end{enumerate}
}}

\end{frame}
